{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39dbad8f-905f-4fbe-ae31-a5fa887f496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b4edca-0f0b-4cf6-a29b-d86b15c63dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "# accounts\n",
    "df_accounts = spark.read \\\n",
    "    .option(\"header\", \"true\")  \\\n",
    "    .option(\"inferSchema\", \"true\")  \\\n",
    "    .csv(\"accounts.csv\", sep=\";\")\n",
    "\n",
    "df_accounts.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d06567-d7fe-4ca5-8b0d-502b554bd552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# country_abbreviation\n",
    "df_country_abbreviation = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"country_abbreviation.csv\", sep=\";\") \n",
    "\n",
    "df_country_abbreviation.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575f87e2-836f-4b6d-8ec3-83fc7e2b3b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# transactions\n",
    "df_transactions = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"transactions.csv\", sep=\";\")\n",
    "\n",
    "df_transactions.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28bf93c-ef36-4d67-94d1-685b20c88c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bf58f-54e1-42d8-b3f6-a9d6ac4e8bb5",
   "metadata": {},
   "source": [
    "• Calculate how many accounts of each type there are using Spark SQL. The return type is a dataframe [account_type: string, account_type_count: int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7719dc5a-d633-408e-b584-a57edbee8649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|account_type|account_type_count|\n",
      "+------------+------------------+\n",
      "|    Personal|           1667072|\n",
      "|Professional|           1667358|\n",
      "|    Business|           1665570|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"AccountTypeCount\").getOrCreate()\n",
    "\n",
    "df_transactions = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"transactions.csv\", sep=\";\")\n",
    "\n",
    "df_accounts = spark.read \\\n",
    "    .option(\"header\", \"true\")  \\\n",
    "    .option(\"inferSchema\", \"true\")  \\\n",
    "    .csv(\"accounts.csv\", sep=\";\")\n",
    "\n",
    "df_accounts.createOrReplaceTempView(\"accounts\")\n",
    "\n",
    "df_transactions.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "df_accounts_result = spark.sql(\"\"\"\n",
    "    SELECT t.account_type AS account_type, COUNT(t.account_type) AS account_type_count \n",
    "    FROM transactions t\n",
    "    LEFT JOIN accounts a\n",
    "    ON a.id = t.id\n",
    "    GROUP BY t.account_type\n",
    "    \"\"\")\n",
    "\n",
    "df_accounts_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da2dfc-8e78-4938-b2c5-ab45edd0d2f3",
   "metadata": {},
   "source": [
    " •\tCalculate only the balance and the latest date for each account from transactions.csv. To calculate the balance, summarize all the transactions for  \n",
    "    each account. The return type is a dataframe [account_id: string, balance: string, latest_date: date]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc170b24-2d58-45c2-b1f6-6739eae97c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def analyze_transaction_data():\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Transaction Data Analysis\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read transactions data from CSV\n",
    "    transactions_df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"transactions.csv\", sep=\";\")\n",
    "\n",
    "    # Create a temporary view for the transactions DataFrame\n",
    "    transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "    # Analyze transaction data to get account balances and latest dates\n",
    "    accounts_last_date_result_df = spark.sql(\"\"\"\n",
    "        SELECT id AS account_id,\n",
    "               CAST(SUM(amount) AS STRING) AS balance,\n",
    "               MAX(transaction_date) as latest_date\n",
    "        FROM transactions \n",
    "        GROUP BY id\n",
    "    \"\"\")\n",
    "\n",
    "    # Show the results\n",
    "    accounts_last_date_result_df.show()\n",
    "\n",
    "analyze_transaction_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3fe10-8caf-4fb7-998b-206269931152",
   "metadata": {},
   "source": [
    "2.\tWrite a function using Spark Python or Spark Scala API to calculate total earnings (sum of transactions above 0) for each user from Switzerland by year as a pivot table. The result dataframe should contain user full names as one field split by whitespace, years, and earning values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba0501c-028f-45b3-a0a4-f45562876734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import sum, col, split, concat_ws\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_transactions = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"transactions.csv\", sep=\";\")\n",
    "\n",
    "# country_abbreviation\n",
    "df_country_abbreviation = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"country_abbreviation.csv\", sep=\";\") \n",
    "\n",
    "\n",
    "df_accounts = spark.read \\\n",
    "    .option(\"header\", \"true\")  \\\n",
    "    .option(\"inferSchema\", \"true\")  \\\n",
    "    .csv(\"accounts.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "def get_total_earnings():\n",
    "    df_transactions_1 = df_transactions \\\n",
    "        .join(df_country_abbreviation,\n",
    "             df_transactions.country == df_country_abbreviation.abbreviation,\n",
    "             \"inner\") \\\n",
    "        .join(df_accounts.drop(\"country\"), \n",
    "             df_transactions.id == df_accounts.id,\n",
    "             \"inner\")\n",
    "\n",
    "    df_filtered_1 = df_transactions_1.filter((col(\"country\") == \"CH\") & (col(\"amount\") > 0))\n",
    "    df_filtered_2 = df_filtered_1.withColumn(\"year\", split(col(\"transaction_date\"), \"-\")[0].cast(\"int\"))\n",
    "    df_filtered_3 = df_filtered_2.withColumn(\"full_name\", concat_ws(\" \", df_filtered_2.first_name, df_filtered_2.last_name))\n",
    "\n",
    "    df_partitioned = df_filtered_3.repartition(\"country\")\n",
    "    \n",
    "    df_result = df_partitioned.groupBy(\"full_name\").pivot(\"year\").sum(\"amount\")\n",
    "    df_result.show()\n",
    "\n",
    "\n",
    "get_total_earnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9994334-f040-441a-b08f-97e71bf8d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "2.\tWrite a function that expects a transactions dataset as input and returns it with an additional column \"level\". The value of \"level \" is calculated based on the \"amount\" column as:\n",
    "•\tTop 25% of all transactions get a value \"high\".\n",
    "•\tThe next 50% of all transactions get \"average\".\n",
    "•\tThe rest gets \"low\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f317d0f8-807c-4019-93b4-e9b77f22550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-------+-------+-----+\n",
      "|    id|account_type| amount|country|level|\n",
      "+------+------------+-------+-------+-----+\n",
      "|179528|    Business|-730.86|     SV| high|\n",
      "|378343|    Personal|-946.98|     YE| high|\n",
      "| 75450|Professional|7816.92|     SI| high|\n",
      "|357719|    Business| 704.02|     ID| high|\n",
      "|110511|    Personal| 3462.6|     BS| high|\n",
      "|461830|Professional| 762.81|     CN| high|\n",
      "| 30180|Professional|5390.24|     GN| high|\n",
      "| 65398|    Personal|4765.77|     TR| high|\n",
      "|170899|    Business|8775.89|     SK| high|\n",
      "|234300|Professional|8455.18|     LU| high|\n",
      "|208027|    Business| 6244.1|     AE| high|\n",
      "|161212|    Personal|5904.56|     EG| high|\n",
      "|105372|Professional|4079.76|     MT| high|\n",
      "|205321|Professional| 3570.4|     MU| high|\n",
      "|410863|    Business|2328.83|     SR| high|\n",
      "|486752|Professional| 5454.8|     CU| high|\n",
      "|208564|    Personal|8695.17|     IT| high|\n",
      "|196682|    Personal|-905.87|     HU| high|\n",
      "|491196|Professional|8781.02|     IR| high|\n",
      "|108286|    Personal|3485.95|     ZW| high|\n",
      "+------+------------+-------+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when,lit, monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import broadcast, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define schema for the Row\n",
    "schema = StructType([\n",
    "    StructField(\"total\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# transactions\n",
    "df_transactions = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"transactions.csv\", sep=\";\")\n",
    "\n",
    "def get_level_by_percent(data_set):\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Get Level by Percent\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Read transactions data\n",
    "    df_transactions = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"transactions.csv\", sep=\";\")\n",
    "\n",
    "    # Count the total number of rows in the dataset\n",
    "    total_count = data_set.count()\n",
    "\n",
    "    # Broadcast the total count\n",
    "    broadcast_df_small = broadcast(spark.createDataFrame([Row(total=total_count)]))\n",
    "\n",
    "    # Order the data by \"amount\" in descending order\n",
    "    data_set_ordered = data_set.orderBy(\"amount\", ascending=False)\n",
    "\n",
    "    # Join with broadcasted total count and add an \"order\" column\n",
    "    data_set_positioned = data_set_ordered.join(broadcast_df_small).withColumn(\"order\", monotonically_increasing_id())\n",
    "\n",
    "    # Calculate the \"level\" based on the order and total count\n",
    "    data_set_positioned.withColumn(\"level\",\n",
    "        when(((col(\"order\") / col(\"total\")) * 100) <= lit(25), \"high\") \\\n",
    "        .when(((col(\"order\") / col(\"total\")) * 100) <= lit(50), \"average\") \\\n",
    "        .otherwise(\"low\")\n",
    "    ).select(\"id\", \"account_type\", \"amount\", \"country\", \"level\").show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_level_by_percent(df_transactions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76003fd8-3c10-4f8d-b8ed-23a0e19975c5",
   "metadata": {},
   "source": [
    "2.\tWrite a function that reads a text file in the format presented below, and then ensure that it returns a data frame of table content with proper columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97b44720-248a-483f-aaa9-af2ec83cbf47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|              Col1| Col2|\n",
      "+---+------------------+-----+\n",
      "|  1|     one,two,three|  one|\n",
      "|  2|     four,one,five|  six|\n",
      "|  3|seven,nine,one,two|eight|\n",
      "|  4|    two,three,five| five|\n",
      "|  5|      six,five,one|seven|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, regexp_replace\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def get_format_file():\n",
    "    # Read the text file\n",
    "    file_read = spark.read.text(\"format_file.txt\")\n",
    "\n",
    "    # Clean and format the data\n",
    "    file_cleaned = file_read.withColumn(\"first_format\", regexp_replace(col(\"value\"), \"-\", \"\"))\n",
    "    file_formatted = file_cleaned.withColumn(\"final_format\", regexp_replace(col(\"first_format\"), \"\\\\+\", \"\")) \\\n",
    "        .filter(col(\"final_format\") != lit(\"\"))\n",
    "\n",
    "    # Get the header and convert the DataFrame to Pandas\n",
    "    file_header = file_formatted.first()\n",
    "    df_pandas = file_formatted.filter(col(\"final_format\") != file_header[\"final_format\"]) \\\n",
    "        .select(\"final_format\").toPandas()\n",
    "\n",
    "    # Split the final_format column and rename columns\n",
    "    df = df_pandas['final_format'].str.split('|', expand=True)\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    df = df.drop(df.columns[-1], axis=1)\n",
    "    df.rename(columns={1: 'id', 2: 'Col1', 3: 'Col2'}, inplace=True)\n",
    "\n",
    "    # Create a Spark DataFrame and show the result\n",
    "    final_df = spark.createDataFrame(df)\n",
    "    final_df.show()\n",
    "\n",
    "# Call the function to execute the code\n",
    "get_format_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ecc4e-eed4-4101-a13c-810043815a75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
